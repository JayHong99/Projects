{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>영상이나 음악이 이쁘다 해도 미화시킨 불륜일뿐</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>히치콕이 이 영화를 봤다면 분명 박수를 쳤을듯...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>괜찮은 음악영화가 또 나왔군요!!! 따뜻한 겨울이 될 것 같아요~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>아무래도 20년도지난작품이라 지금보기는너무유치하다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>지금까지의 영화들이 그랬듯. 이 영화역시 일본에 대한 미화는 여전하다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                 document  label\n",
       "0   1                영상이나 음악이 이쁘다 해도 미화시킨 불륜일뿐      0\n",
       "1   2             히치콕이 이 영화를 봤다면 분명 박수를 쳤을듯...      1\n",
       "2   3     괜찮은 음악영화가 또 나왔군요!!! 따뜻한 겨울이 될 것 같아요~      1\n",
       "3   4              아무래도 20년도지난작품이라 지금보기는너무유치하다      0\n",
       "4   5  지금까지의 영화들이 그랬듯. 이 영화역시 일본에 대한 미화는 여전하다.      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터의 특징 살펴보기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1) 데이터의 양\n",
    "    train 5,000행과 test 5,000행밖에 없습니다.\n",
    "    데이터가 굉장히 적은 축에 속합니다.    \n",
    "    이럴 때에는 모델링 과정에서 Cross Validation Score로 검증합니다.\n",
    "    이 과정에서, train과 test의 비율은 어느정도 일정하니 Stratify 는 사용하지 않겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 3), (5000, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2564\n",
       "1    2436\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 데이터의 형태\n",
    "    데이터에 굉장히 많은 특수문자가 있습니다. => 정규표현식을 이용해 제거\n",
    "    조사는 데이터 분석에 악영향을 끼칠 것으로 보입니다 => 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영상이나 음악이 이쁘다 해도 미화시킨 불륜일뿐 ==> 영상이나 음악이 이쁘다 해도 미화시킨 불륜일뿐\n",
      "히치콕이 이 영화를 봤다면 분명 박수를 쳤을듯... ==> 히치콕이 이 영화를 봤다면 분명 박수를 쳤을듯\n",
      "괜찮은 음악영화가 또 나왔군요!!! 따뜻한 겨울이 될 것 같아요~ ==> 괜찮은 음악영화가 또 나왔군요 따뜻한 겨울이 될 것 같아요\n"
     ]
    }
   ],
   "source": [
    "def return_clean_text(x) : \n",
    "    import re\n",
    "    x = re.sub('[^ ㄱ-ㅣ가-힣+]', ' ', x) # ㄱ에서 ㅣ 그리고 가-힣 이외의 단어는 띄어쓰기로 변호나\n",
    "    x = re.sub(' +', ' ', x) # ...같은 단어는 띄어쓰기x3 으로 바뀌기 때문에, 하나로 병합\n",
    "    return x.strip()\n",
    "train['new_doc'] = train['document'].apply(return_clean_text)\n",
    "test['new_doc'] = test['document'].apply(return_clean_text)\n",
    "\n",
    "for i in range(3) : \n",
    "    print(train['document'][i], '==>', train['new_doc'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['은','는','이','가','을','를','다','나','까','요']\n",
    "\n",
    "def return_clean_text2(x) : \n",
    "    new_txt = []\n",
    "    for word in x.split() : \n",
    "        try : \n",
    "            while word[-1] in stop_words : \n",
    "                word = word[:-1]\n",
    "            new_txt += [word]\n",
    "        except : pass;\n",
    "        \n",
    "    new_txt = ' '.join(new_txt)\n",
    "    return new_txt\n",
    "\n",
    "train['clean_doc'] = train['new_doc'].apply(return_clean_text2)\n",
    "test['clean_doc'] = test['new_doc'].apply(return_clean_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영상이나 음악이 이쁘다 해도 미화시킨 불륜일뿐 ==> 영상이나 음악이 이쁘다 해도 미화시킨 불륜일뿐 ==> 영상 음악 이쁘 해도 미화시킨 불륜일뿐\n",
      "히치콕이 이 영화를 봤다면 분명 박수를 쳤을듯... ==> 히치콕이 이 영화를 봤다면 분명 박수를 쳤을듯 ==> 히치콕 영화 봤다면 분명 박수 쳤을듯\n",
      "괜찮은 음악영화가 또 나왔군요!!! 따뜻한 겨울이 될 것 같아요~ ==> 괜찮은 음악영화가 또 나왔군요 따뜻한 겨울이 될 것 같아요 ==> 괜찮 음악영화 또 나왔군 따뜻한 겨울 될 것 같아\n"
     ]
    }
   ],
   "source": [
    "for i in range(3) : \n",
    "    print(train['document'][i], '==>', train['new_doc'][i], '==>' , train['clean_doc'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer(TF-Vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Count Vectorizer는 단순하게 Token 에 대한 \"빈도수\"를 계산합니다.\n",
    "    해당 단어가 몇번이 나왔는지에 대한 \"횟수\"를 의미합니다.\n",
    "    따라서 Count Vectorizer를 이용할 때는 유의할 것과 설정할 것들이 몇가지 있습니다.\n",
    "    \n",
    "    1. N-gram\n",
    "        - N-gram이라는 것은 몇개의 단어를 이어 붙여 하나의 Token으로 볼지에 대한 개념입니다.\n",
    "        - Example) 나는 오늘 밥을 먹었다. , 띄어쓰기 단위로 분류\n",
    "            - uni-gram(N=1) : \"나는\" , \"오늘\" , \"밥을\" , \"먹었다\"\n",
    "            - bi-gram (N=2) : \"나는 오늘\", \"오늘 밥을\", \"밥을 먹었다\"\n",
    "            - etc...\n",
    "        - 특징\n",
    "            - 개별 단어만 가지고는 알지 못하는 특성을 계산합니다.\n",
    "            - example) \"좋다\" 와 \"안 좋다\"는 완전 별개의 개념인데, 이를 잡아냅니다\n",
    "        - 문제점\n",
    "            - 단어의 수가 늘어납니다.\n",
    "            - 기본적으로 단어가 K개 있다면, kCn 개 만큼 생겨납니다. (n 은 gram 설정 값)\n",
    "            - 그에 따른 Feature가 증가하므로, 계산량이 증폭됩니다.\n",
    "        - 선택\n",
    "            - 이번 test에서는 n_gram 범주를 (1,2)로 설정하여, 개별 token + bigram token 가지 봅니다.\n",
    "\n",
    "    2. 이상치 제어\n",
    "        - 기존 문제점\n",
    "            - 굉장히 많은 빈도로 등장하는 단어들이 있을 수 있습니다.\n",
    "            - Example) IMDB의 특성상 \"영화\"라는 단어는 굉장히 많이 나옵니다.\n",
    "        - 선택\n",
    "            - 데이터가 적어 별도의 설정은 하지 않습니다.\n",
    "    \n",
    "\n",
    "    3. Vectorizer Dimmension 설정\n",
    "        - 기존 문제점\n",
    "            - 모든 단어에 대한 빈도수를 측정하다보니, 모든 개별 단어를 하나의 차원으로 만듭니다.\n",
    "            - 이는 \"차원의 저주\"로 이어질 수 있습니다.\n",
    "                - 이를 해결 하기 위해서는 다양한 방법이 있지만, 여기서는 다루지 않겠습니다.\n",
    "        - 선택\n",
    "            - 저는 1만, 2만, 3만 단어에 대해서 간단한 MLP 모델의 Validation 점수를 비교하고, 최고의 점수를 가지는 모델을 선정하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module) : \n",
    "\n",
    "    def __init__(self, max_f) : \n",
    "        super().__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(max_f, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input) : \n",
    "        output = self.main(input)\n",
    "        return output.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, criterion, optimizer, num_epochs) : \n",
    "    import copy\n",
    "\n",
    "    train_history = []\n",
    "    valid_history = []\n",
    "    best_model_weigths = copy.deepcopy(model.state_dict)\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(num_epochs) :\n",
    "        print('='*20)\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "\n",
    "        for phase in ['train','valid'] : \n",
    "            if phase == 'train' : \n",
    "                model.train()\n",
    "            else : \n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_acc = 0\n",
    "\n",
    "            for inputs, labels in data[phase] : \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train') : \n",
    "                    outputs = model(inputs)\n",
    "                    print(outputs.shape)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train' : \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_acc += (outputs == labels).sum()\n",
    "            epoch_loss = running_loss / len(data[phase])\n",
    "            epoch_acc = running_acc / len(data[phase]) \n",
    "            epoch_acc_str = str(int(round(0.982, 2) * 100))+'%'\n",
    "            print(f'{phase} Loss: {round(epoch_loss,4)}')\n",
    "            print(f'{phase} Acc : {epoch_acc_str}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_history.append([epoch_loss, epoch_acc])\n",
    "            \n",
    "            if phase == 'val':\n",
    "                valid_history.append([epoch_loss, epoch_acc])\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_val_acc:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_val_acc = epoch_acc\n",
    "                best_model_weigths = copy.deepcopy(model.state_dict())\n",
    "        print()\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_weigths)\n",
    "\n",
    "    return model, train_history, valid_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = [10000, 20000, 30000]\n",
    "\n",
    "for max_f in max_features : \n",
    "    cv = CountVectorizer(ngram_range = (1,2), max_features = max_f)\n",
    "    cv_train = cv.fit_transform(train['new_doc']).toarray()\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "data['train'] = [[\n",
    "                    torch.from_numpy(cv_train[:4000]).float(), \n",
    "                    torch.from_numpy(train['label'].values[:4000]).float(), \n",
    "                    \n",
    "                ]]\n",
    "data['valid'] = [[\n",
    "                \n",
    "                    torch.from_numpy(cv_train[4000:]).float(),\n",
    "                    torch.from_numpy(train['label'].values[4000:]).float(), \n",
    "                    \n",
    "                ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Epoch 0/9\n",
      "torch.Size([4000])\n",
      "train Loss: 2801.1088\n",
      "train Acc : 98%\n",
      "torch.Size([1000])\n",
      "valid Loss: 701.0533\n",
      "valid Acc : 98%\n",
      "\n",
      "====================\n",
      "Epoch 1/9\n",
      "torch.Size([4000])\n",
      "train Loss: 2798.4133\n",
      "train Acc : 98%\n",
      "torch.Size([1000])\n",
      "valid Loss: 700.4146\n",
      "valid Acc : 98%\n",
      "\n",
      "====================\n",
      "Epoch 2/9\n",
      "torch.Size([4000])\n",
      "train Loss: 2796.1683\n",
      "train Acc : 98%\n",
      "torch.Size([1000])\n",
      "valid Loss: 699.7957\n",
      "valid Acc : 98%\n",
      "\n",
      "====================\n",
      "Epoch 3/9\n",
      "torch.Size([4000])\n",
      "train Loss: 2794.0207\n",
      "train Acc : 98%\n",
      "torch.Size([1000])\n",
      "valid Loss: 699.1997\n",
      "valid Acc : 98%\n",
      "\n",
      "====================\n",
      "Epoch 4/9\n",
      "torch.Size([4000])\n",
      "train Loss: 2791.945\n",
      "train Acc : 98%\n",
      "torch.Size([1000])\n",
      "valid Loss: 698.622\n",
      "valid Acc : 98%\n",
      "\n",
      "====================\n",
      "Epoch 5/9\n",
      "torch.Size([4000])\n",
      "train Loss: 2789.928\n",
      "train Acc : 98%\n",
      "torch.Size([1000])\n",
      "valid Loss: 698.061\n",
      "valid Acc : 98%\n",
      "\n",
      "====================\n",
      "Epoch 6/9\n",
      "torch.Size([4000])\n",
      "train Loss: 2787.9694\n",
      "train Acc : 98%\n",
      "torch.Size([1000])\n",
      "valid Loss: 697.4977\n",
      "valid Acc : 98%\n",
      "\n",
      "====================\n",
      "Epoch 7/9\n",
      "torch.Size([4000])\n",
      "train Loss: 2785.9566\n",
      "train Acc : 98%\n",
      "torch.Size([1000])\n",
      "valid Loss: 696.8826\n",
      "valid Acc : 98%\n",
      "\n",
      "====================\n",
      "Epoch 8/9\n",
      "torch.Size([4000])\n",
      "train Loss: 2783.5548\n",
      "train Acc : 98%\n",
      "torch.Size([1000])\n",
      "valid Loss: 696.0486\n",
      "valid Acc : 98%\n",
      "\n",
      "====================\n",
      "Epoch 9/9\n",
      "torch.Size([4000])\n",
      "train Loss: 2779.6381\n",
      "train Acc : 98%\n",
      "torch.Size([1000])\n",
      "valid Loss: 694.7423\n",
      "valid Acc : 98%\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-812d5286e01d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-b6d747ccaab3>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, data, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# load best model weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model_weigths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m         \u001b[1;31m# copy state_dict so _load_from_state_dict can modify it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_metadata'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m         \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m             \u001b[1;31m# mypy isn't aware that \"_metadata\" exists in state_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "model = mlp(max_f).to(device)\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "train_model(model, data, loss_func, optimizer, 10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
